<!-- cSpell:ignore discogrande,curtin,desc谩rgalo,descompr铆melo, dfadmin,netplan,subiquity,hadoop,redhat,subtag,esbenp,ethernets,ecdsa,openssh,fdisk,namenode,datanode,decomisionado,hdfs,dfsadmin,reconfig,fsimage,datanodes,httpfs,jobhistory,namenodes,jconsole,getconf,Xceivers,pagebreak,Diffie-Hellman,
-->

# bda02-02-notes

## red

### red - conceptos

### red - archivos

####  `/etc/netplan/00-installer-config.yaml` [root]

> Configurar las interfaces de red, definir ip mascara red en cada una de las mismas

```bash
# This is the network config written by 'subiquity'
# nota bie1 - 192.168.0.101
# nota bie2 - 192.168.0.102
# nota bie3 - 192.168.0.103
# nota bie4 - 192.168.0.104
network:
  ethernets:
    enp0s3:
      addresses:
        - 192.168.0.101/24
      routes:
        - to: default
          via: 192.168.0.1
      nameservers:
        addresses:
          - 8.8.8.8
  version: 2
```

###  `/etc/hosts` [root]

> definir "dns" local

```bash
127.0.0.1 localhost

# hosts 192.168.0.10X bie(n)
192.168.0.100   bieTemplate
192.168.0.101   bie1
192.168.0.102   bie2
192.168.0.103   bie3
192.168.0.104   bie4
# [...]
```

####  `/etc/hostname` [root]

> nombre del equipo

```bash
# nota bie1
# nota bie2
# nota bie3
# nota bie4
bie1
```

### red - comandos

#### basic

```bash
ip a # ver direcci贸n ip y mascara
ip route #consultar tabla de rutas
sudo netplan try #
```

#### Verificar configuraci贸n de los archivos

```bash
echo "[/etc/netplan/00-installer-config.yaml]";
cat /etc/netplan/00-installer-config.yaml ;
echo "[/etc/hosts]"; head /etc/hosts ;
echo "[/etc/hostname]" ;cat /etc/hostname
```

#### verificar comunicaci贸n entre m.v

```bash
ping -q -c 2 bie1 ;
ping -q -c 2 -v bie2;
ping -q -c 2 bie3;
ping -q -c 2 bie4;
ping -q -c 1 8.8.8.8;
ping -q -c1 google.com
```

## shh

### ssh - conceptos

#### ssh basic

Por ejemplo, si el nombre de usuario es "usuario", las claves p煤blicas por defecto estar谩n en:
|algoritmo de creaci贸n | clave publica | clave privada |
|-|-|-|
|RSA | /home/usuario/.ssh/id_rsa.pub | /home/usuario/.ssh/id_rsa |
|DSA | /home/usuario/.ssh/id_dsa.pub | /home/usuario/.ssh/id_dsa |
|ECDSA | /home/usuario/.ssh/id_ecdsa.pub | /home/usuario/.ssh/id_ecdsa |
|Ed25519 | /home/usuario/.ssh/id_ed25519.pub | /home/usuario/.ssh/id_ed25519 |

crear clave publica y privada

| concepto   | explicaci贸n                                              |
| ---------- | -------------------------------------------------------- |
| ssh-keygen | `ssh-keygen -t rsa -f /home/hadoop/.ssh/bie1`            |
| -t         | algoritmo usado                                          |
| -f         | ubicaci贸n del directorio para las claves ssh del usuario |

para no tener que introducir una contrase帽a, podemos copiar la clave publica en otros nodos

crear clave publica y privada

| concepto    | explicaci贸n                                                                               |
| ----------- | ----------------------------------------------------------------------------------------- |
| ssh-copy-id | `ssh-copy-id -i /home/hadoop/.ssh/bie1.pub hadoop@bie2`                                   |
| -i          | especifica la ubicaci贸n del archivo de clave p煤blica que deseas copiar al servidor remoto |
| $1          | direcci贸n del servidor remoto al que deseas copiar la clave p煤blica                       |

#### A nivel de Usuario

`ls -la ~/.ssh/` --> directorio config. ssh a nivel de sistema

entendiendo `nodo`como nombre definido

`archivo *nodo` --> Clave privada
`archivo *nodo.pub` --> Clave publica
`known_hosts` almacena las claves p煤blicas de los servidores a los que un usuario se ha conectado anteriormente a trav茅s del protocolo SSH.
`archivo authorized_keys`
especificar qu茅 claves p煤blicas de SSH est谩n autorizadas para acceder a una cuenta de usuario espec铆fica a trav茅s de SSH sin necesidad de proporcionar una contrase帽a.
Cuando un usuario intenta acceder a un servidor SSH, el servidor verifica si la clave p煤blica utilizada por el cliente se encuentra en el archivo authorized_keys correspondiente a la cuenta de usuario a la que se est谩 intentando acceder. Si la clave p煤blica del cliente coincide con alguna de las claves p煤blicas listadas en authorized_keys, el acceso se permite sin requerir una contrase帽a.

#### A nivel de Sistema

`/etc/ssh/sshd_config` --> directorio config. ssh a nivel de sistema

##### !! important nivel usuario

`/etc/ssh/ssh_config`

> Este archivo contiene la configuraci贸n global para clientes SSH. Define opciones de configuraci贸n predeterminadas para la ejecuci贸n de comandos ssh desde la l铆nea de comandos.

- `La opci贸n IdentityFile`
  se usa para especificar la ubicaci贸n de la clave privada que el cliente SSH utilizar谩 durante la autenticaci贸n con el servidor SSH.

`ssh_config.d`

Este directorio puede contener archivos adicionales de configuraci贸n para clientes SSH. Estos archivos se incluyen y se aplican junto con el ssh_config principal.

##### !! important nivel servidor

`sshd_config.d`

Similar al directorio ssh_config.d, este directorio puede contener archivos adicionales de configuraci贸n para el servidor SSH. Estos archivos se incluyen y se aplican junto con el sshd_config principal.

`sshd_config`
Este archivo contiene la configuraci贸n del servidor SSH. Define opciones como el puerto de escucha, la autenticaci贸n permitida, configuraci贸n de claves, etc.

##### others

`moduli`:

Este archivo contiene los par谩metros de los grupos de moduli utilizados para la negociaci贸n de claves Diffie-Hellman en SSH. Estos par谩metros afectan la seguridad de la conexi贸n.

`ssh_host_ecdsa_key y ssh_host_ecdsa_key.pub`

Estos archivos contienen la clave privada y p煤blica, respectivamente, para el algoritmo de clave ECDSA (Elliptic Curve Digital Signature Algorithm) utilizado por el servidor SSH para autenticaci贸n y establecimiento de claves seguras.

`ssh_host_ed25519_key y ssh_host_ed25519_key.pub`

Contienen la clave privada y p煤blica, respectivamente, para el algoritmo de clave Ed25519 utilizado por el servidor SSH.

`ssh_host_rsa_key y ssh_host_rsa_key.pub`

Contienen la clave privada y p煤blica, respectivamente, para el algoritmo de clave RSA utilizado por el servidor SSH.

`ssh_host_dsa_key y ssh_host_dsa_key.pub`

Contienen la clave privada y p煤blica, respectivamente, para el algoritmo de clave DSA (Digital Signature Algorithm) utilizado por el servidor SSH.

`ssh_import_id`

Este archivo se utiliza para importar identidades de claves p煤blicas a la base de datos de autenticaci贸n SSH.

### ssh - archivos

 /etc/ssh/ssh_config [root]

```bash
IdentityFile ~/.ssh/nodo1
```

### ssh - comandos

#### instal ssh

```bash
# instalar ssh server
sudo apt install openssh-server
```

#### create private and publics keys

```bash
# crear pareja de llaves publica y privada
ssh-keygen
/home/hadoop/.shh/nodo2
# enter dejar contrase帽a en blanco
# enter dejar repetir contrase帽a en blanco
ls -la .ssh/ # verificar

# en un comando
ssh-keygen -t rsa -f /home/hadoop/.ssh/bie1 ; ls -la .ssh/
ssh-keygen -t rsa -f /home/hadoop/.ssh/bie2 ; ls -la .ssh/
ssh-keygen -t rsa -f /home/hadoop/.ssh/bie3 ; ls -la .ssh/
ssh-keygen -t rsa -f /home/hadoop/.ssh/bie4 ; ls -la .ssh/
```

#### definir ubicaci贸n de la clave privada que el cliente SSH

```bash
 sudo nano /etc/ssh/ssh_config
 IdentityFile ~/.ssh/nodo1 # escribir al final del archivo
 ## un solo comando
 sudo bash -c 'echo "IdentityFile ~/.ssh/bie1" >> /etc/ssh/ssh_config'
 sudo bash -c 'echo "IdentityFile ~/.ssh/bie2" >> /etc/ssh/ssh_config'
 sudo bash -c 'echo "IdentityFile ~/.ssh/bie3" >> /etc/ssh/ssh_config'
 sudo bash -c 'echo "IdentityFile ~/.ssh/bie4" >> /etc/ssh/ssh_config'
 ## verificar
 cat /etc/ssh/ssh_config | grep "IdentityFile"
```

#### shared public keys to other nodes

```bash
#nodo1
ssh-copy-id -i /home/hadoop/.ssh/bie1.pub hadoop@bie1
# lo mismo con los dem谩s
ssh-copy-id -i /home/hadoop/.ssh/bie1.pub hadoop@bie2
ssh-copy-id -i /home/hadoop/.ssh/bie1.pub hadoop@bie3
ssh-copy-id -i /home/hadoop/.ssh/bie1.pub hadoop@bie4
# un solo comando
for nodo in bie1 bie2 bie3 bie4;
do ssh-copy-id -i /home/hadoop/.ssh/bie1.pub hadoop@$nodo;
done
```

#### copia de archivos

```bash
# copiar carpeta hadoop a los otros nodos
scp -r /home/hadoop/hadoop hadoop@bie2:/home/hadoop
```

## disco

### disco - conceptos

### disco - archivos

 /etc/fstab [root]

> Persistencia ruta de montage

```bash
# /boot was on /dev/sda2 during curtin installation
/dev/disk/by-uuid/de5fd9a1-953c-4dfe-a534-be2976a839b1 /boot ext4 defaults 0 1
/swap.img       none    swap    sw      0       0
# montaje persistente de disco grande
# <file system> <mount point>                   <type>  <options>  <dump>  <pass>
/dev/sbd1       /home/hadoop/discogrande        ext4    defaults     0       0
```

### disco - comandos

```bash
# --- Paso a paso --- #
lsblk
sudo fdisk /dev/sbd
p # imprimo la info del disco
g # cambio a gpt
n # creo una nueva partici贸n
# intro
# intro
# intro
w # escribir los cambios
sudo mkfs.ext4 /dev/sbd1
# crear ruta de montaje
mkdir discogrande
# montar partici贸n
sudo mount /dev/sbd1 /home/hadoop/discogrande/
# persistencia
 sudo nano /etc/fstab
# a帽adir --> /dev/sbd1    /home/hadoop/discogrande      ext4  defaults    0     0
# cambiar permisos a usuario hadoop
sudo chown -R hadoop:hadoop /home/hadoop/discogrande

# --- en un solo comando --- #
echo -e "g\nn\n\n\n\nw\n" | sudo fdisk /dev/sbd;
sudo mkfs.ext4 /dev/sbd1;
mkdir discogrande;
sudo mount /dev/sbd1 /home/hadoop/discogrande/;
sudo sh -c 'printf "/dev/sbd1\t/home/hadoop/discogrande\text4\tdefaults\t0\t0\n">> /etc/fstab';

sudo chown -R hadoop:hadoop /home/hadoop/discogrande;
# --- Verificar --- #
lsblk | grep "discogrande";
ls -la | grep "discogrande";
```

### hadoop

### hadoop - conceptos

**_ core-site.xml [namenode]_**
|propiedad| descripci贸n|
|-|-|
|`dfs.namenode.name.dir`|Ruta donde esta guarda la tabla de direcciones de namenode|
|`dfs.replication`| Cuando guardas un archivo en Hadoop, el archivo se divide en fragmentos de datos llamados "bloques" , pore defecto (128 MB). estos bloques se guardan en n nodos siendo n el valor de dfs.replication, en este caso como el valor es 2 se guarda cada "bloque" en 2 nodos.|
|act煤a|en este archivo el nodo1 act煤a como namenode, por lo cual unicamente definimos en la configuraci贸n relativa al proceso de namenode, mientras que nodo2,3,4 van a actuar谩 como datanode y definimos en ello de manera id茅ntica la configuraci贸n relativa a datanode|

**_ core-site.xml [datanode]_**

**nodo - modo dead**

para que hadoop declare un nodo muerto:";
1.- tiene que pasar un nodo sin responder 10 latidos de [dfs.heartbeat.interval] (3 x defecto) segundos ";
2.- dos periodos de gracia de [dfs.namenode.heartbeat.recheck-interval] (5 min por defecto) cada uno";
esto ocurre para asegurarse plenamente de que el nodo no es funcional antes de que se muevan cantidades muy grandes de informaci贸n implicando la saturaci贸n de recursos ";

**nodo - modo mantenimiento**

**nodo - modo decomisionado**

operaci贸n de mantenimiento de gran duraci贸n
ejemplo cambiar todos los m贸dulos del rack

### hadoop - archivos

 `nano /home/hadoop/hadoop/etc/hadoop/core-site.xml`
驴Qu茅 estamos indicando en esa configuraci贸n?

> Este archivo ` core-site.xml` se utiliza para especificar la configuraci贸n centralizada para el sistema de archivos y otros aspectos fundamentales del cl煤ster de Hadoop

| type      | Concepto           | descripci贸n                                                                                                                                                            |
| --------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Propiedad | `fs.defaultFS`     | Propiedad que define el sistema de archivos por defecto que utilizar谩 Hadoop                                                                                           |
| value     | `hdfs://bie1:9000` | `hdfs` Sistema de archivos por defecto utilizado por Hadoop ser谩 HDFS (Hadoop Distributed File System) </br>`bie1` la direcci贸n del namenode </br>`:9000` puerto 9000. |

驴Por qu茅 todos los nodos comparten la la misma configuraci贸n en el archivo core-site.xml?

> En este caso definimos que el nodo bie1 actuara como namenode esta configuraci贸n se aplica en todos los nodos, puesto que es una configuraci贸n global y todos ellos tienen que saber la direcci贸n del nodo que actuara como namenode, es decir donde se guardaran la tabla direcciones.

```xml
<configuration>
<!--
  `fs.defaultFS`
    propiedad que define el sistema de archivos por defecto que utilizar谩 Hadoop , indicamos la direcci贸n del namenode a la que debe conectarse Hadoop
   hdfs://bie1:9000=
  `hdfs`   sistema de archivos por defecto utilizado por Hadoop ser谩 HDFS (Hadoop Distributed File System)
  `bie1` la direcci贸n del namenode
  `:9000`  puerto 9000.
-->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://bie1:9000</value>
  </property>

</configuration>
```

 `/home/hadoop/hadoop/etc/hadoop/hdfs-site.xml` en `namenode`

```xml
<!--
  more configuration property
  https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

  nota:
  al modificar este archivo se puede configurar en caliente
  hdfs dfsadmin -reconfig namenode bie1:9000 start
-->
<configuration>
<!--
  dfs.namenode.name.dir
  file://${hadoop.tmp.dir}/dfs/name
  Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.
  - - - - - - - - - - - - - - - - - - - - - - - -
  Ruta donde esta guarda la table de direcciones de namenode
-->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/hadoop/discogrande/namenode</value>
  </property>
<!--
  dfs.replication
  default 3
  Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.
  - - - - - - - - - - - - - - - - - - - - - - - -
  Cuando guardas un archivo en Hadoop, el archivo se divide en fragmentos de datos llamados "bloques" , pore defecto (128 MB). estos bloques se guardan en n nodos siendo n el valor de dfs.replication, en este caso como el valor es 2 se guarda cada "bloque" en 2 nodos.
-->
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
<!--
  dfs.heartbeat.interval
  default 3
  Determines datanode heartbeat interval in seconds.
  - - - - - - - - - -
  tiempo que tarde en mili-segundos en dar un latido
-->
  <property>
    <name>dfs.heartbeat.interval</name>
    <value>1</value>
  </property>
<!--
  dfs.namenode.heartbeat.recheck-interval
  default 	300000
  300000	This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond.
  - - - - - - - - - - - - - - - - - - - - - - - -
  En el proceso de declarar un nodo como muerto
  tras no responder un nodo tras 10 latidos
  hadoop establece w periodos de gracia
  tiempo en mili-segundos de cada periodo de gracia :
-->
  <property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>500</value>
  </property>
<!--
  dfs.hosts
  Names a file that contains a list of hosts that are permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, all hosts are permitted.
  - - - - - - - - - - - - - - - - - - - - - - - -
  En el proceso de declarar un nodo como muerto
  tras no responder un nodo tras 10 latidos
  hadoop establece w periodos de gracia
  tiempo en mili-segundos de cada periodo de gracia :
-->
   <property>
    <name>dfs.hosts</name>
    <value>/home/hadoop/hadoop/etc/hadoop/workers</value>
  </property>

</configuration>
```

 `nano /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml` en `datanode`

```xml
<configuration>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/hadoop/discogrande/datanode</value>
  </property>

</configuration>
```

 `/home/hadoop/hadoop/etc/hadoop/workers`

> Cuando se ejecutan tareas de procesamiento de datos en un cl煤ster de Hadoop, el framework Hadoop consulta este archivo para determinar qu茅 nodos est谩n disponibles para realizar el trabajo.

```bash
# default localhost
bie2
bie3
bie4
```

`/home/hadoop/hadoop/sbin/`

```bash
# ejecutables
distribute-exclude.sh httpfs.sh start-all.cmd start-dfs.sh stop-all.cmd stop-dfs.sh workers.sh
FederationStateStore kms.sh start-all.sh start-secure-dns.sh stop-all.sh stop-secure-dns.sh yarn-daemon.sh
hadoop-daemon.sh mr-jobhistory-daemon.sh start-balancer.sh start-yarn.cmd stop-balancer.sh stop-yarn.cmd yarn-daemons.sh
hadoop-daemons.sh refresh-namenodes.sh start-dfs.cmd start-yarn.sh stop-dfs.cmd stop-yarn.sh
```

### hadoop - comandos

#### configuraci贸n previa hadoop

##### install java

```bash
# install java 8
sudo apt install openjdk-8-jdk
# detectamos ruta de instalaci贸n --> /usr/lib/jvm/java-8-openjdk-amd64
# update-alternatives: utilizando [/usr/lib/jvm/java-8-openjdk-amd64]/bin/jconsole
```

##### install hadoop

```bash
# instalar hadoop
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
## descomprimir
tar -xzf hadoop-3.3.6.tar.gz
ls -la
## acortar nombre y ruta
mv hadoop-3.3.6 hadoop
# copiar carpeta hadoop a los otros nodos
scp -r /home/hadoop/hadoop hadoop@bie2:/home/hadoop

```

##### conf environment

```bash
# configurar entorno
sudo nano /etc/environment
# a帽adir al PATH binarios de hadoop
PATH="[...]:/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin"
# a帽adir variable de entorno HADOOP_HOME
HADOOP_HOME="/home/hadoop/hadoop"
# a帽adir variable de entorno JAVA_HOME
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
# en un solo comando
sudo sh -c 'echo "JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /etc/environment';
sudo sh -c 'echo "HADOOP_HOME=/home/hadoop/hadoop" >> /etc/environment';
sudo sh -c 'echo "PATH=$PATH:/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin" >> /etc/environment'
# verificar
cat /etc/environment | grep "JAVA_HOME";
# verificar instalaci贸n java
java -version
echo $JAVA_HOME; # necesario reboot
cat /etc/environment | grep hadoop
echo $HADOOP_HOME # necesario reboot

```

#### configuraci贸n b谩sica hadoop core-site.xml

```bash
#=======================#
# editamos core-site.xml
#=======================#
# editamos core-site.xml
nano /home/hadoop/hadoop/etc/hadoop/core-site.xml
# no necesitamos permisos de administrador
ls -la /home/hadoop/hadoop/etc/hadoop/core-site.xml
#-rw-r--r-- 1 hadoop hadoop 869 feb 17 20:09 /home/hadoop/hadoop/etc/hadoop/core-site.xml
# a帽adimos entre <configuration> </configuration>
# <property>
#    <name>fs.defaultFS</name>
#    <value>hdfs://bie1:9000</value>
#  </property>

# en un solo comando
# borramos <configuration> </configuration>
sudo sed -i '/<configuration>/d' /home/hadoop/hadoop/etc/hadoop/core-site.xml
&& sudo sed -i '/<\/configuration>/d' /home/hadoop/hadoop/etc/hadoop/core-site.xml
&& sudo bash -c 'echo "<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://bie1:9000</value>
  </property>
</configuration>" >> /home/hadoop/hadoop/etc/hadoop/core-site.xml'

# core-site.xml se mantiene igual en todos los nodos con lo cual podemos copiarlo
scp /home/hadoop/hadoop/etc/hadoop/core-site.xml hadoop@bie2:/home/hadoop/hadoop/etc/hadoop/core-site.xml;
scp /home/hadoop/hadoop/etc/hadoop/core-site.xml hadoop@bie3:/home/hadoop/hadoop/etc/hadoop/core-site.xml;
scp /home/hadoop/hadoop/etc/hadoop/core-site.xml hadoop@bie4:/home/hadoop/hadoop/etc/hadoop/core-site.xml;
```

#### configuraci贸n b谩sica hadoop hdfs-site.xml `namenode`

```bash
# creo namenode en nodo1
mkdir ~/discogrande/namenode

# no necesitamos permisos de administrador
hadoop@bie1:~$ ls -la /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
#-rw-r--r-- 1 hadoop hadoop 979 feb 17 19:04 /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml

# configuramos namenode
# editamos hdfs-site.xml [nodo1]
nano /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
# a帽adimos entre <configuration> </configuration>
#  <property>
#    <name>dfs.namenode.name.dir</name>
#    <value>/home/hadoop/discogrande/namenode</value>
#  </property>
#
#  <property>
#    <name>dfs.replication</name>
#    <value>2</value>
#  </property>
```

#### configuraci贸n b谩sica hadoop hdfs-site.xml `datanode`

```bash
# creo en nodo2,3,4 datanode
mkdir ~/discogrande/datanode

# configuramos datanode
# editamos hdfs-site.xml [nodo2,3,4]
nano /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
# a帽adimos nodo2 entre <configuration> </configuration>
# <property>
#    <name>dfs.datanode.data.dir</name>
#    <value>/home/hadoop/discogrande/datanode</value>
# </property>
# copiamos en nodo3 y nodo4
scp /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml hadoop@bie3:/home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
scp /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml hadoop@bie4:/home/hadoop/hadoop/etc/hadoop/hdfs-site.xml

# verificar que nos muestra la info entre <configuration> </configuration>
echo "[ /home/hadoop/hadoop/etc/hadoop/core-site.xml]";
sed -n '/<configuration>/,/<\/configuration>/p' /home/hadoop/hadoop/etc/hadoop/core-site.xml;
echo "[ /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml]";
sed -n '/<configuration>/,/<\/configuration>/p' /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml;
```

#### configuraci贸n fina

ver configuraci贸n

[web - http://192.168.0.101:9870/](http://192.168.0.101:9870/)

#### hadoop acciones

```bash
# formatear namenode
hdfs namenode -form
# ver procesos java activos
jps
# ver
hdfs dfsadmin -report
# modificar en caliente hadoop/workers
hdfs dfsadmin -refreshNodes
# volver a leer la configuraci贸n de en caliente
hdfs dfsadmin -reconfig namenode bie1:9000 start

# iniciar namenode
hdfs --daemon start namenode
# iniciar datanode
hdfs --daemon start datanode

# consultar valores de configuraci贸n
# nota : hdfs getconf -confKey propiedad que queremos consultar
echo "tabla namenode guardad en la ruta"
hdfs getconf -confKey dfs.namenode.name.dir;
echo "cantidad de replicas que se mantienen"
hdfs getconf -confKey dfs.replication;
echo "frecuencia de latidos en segundos"
hdfs getconf -confKey dfs.heartbeat.interval;
echo "tiempo de espera de los dos periodos de gracia en mili-segundos"
hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval;
hdfs getconf -confKey dfs.hosts;
echo "hay que tener en cuenta :"
echo "para que hadoop declare un nodo muerto:";
echo -e "\t 1.- tiene que pasar  un nodo sin responder 10 latidos de [dfs.heartbeat.interval] (3 x defecto) segundos ";
echo -e "\t 2.- dos periodos de gracia de [dfs.namenode.heartbeat.recheck-interval] (5 min default) cada uno";
echo "esto ocurre para asegurarse plenamente de que el nodo no es funcional antes de que se muevan cantidades muy grandes de informaci贸n implicando la saturaci贸n de recursos ";

```

#### hadoop salida de comandos

salida de `hdfs dfsadmin -report`

```txt

Configured Capacity: 315264577536 (293.61 GB)
Present Capacity: 299108155392 (278.57 GB)
DFS Remaining: 299108081664 (278.57 GB)
DFS Used: 73728 (72 KB)
DFS Used%: 0.00%
Replicated Blocks:
        Under replicated blocks: 0
        Blocks with corrupt replicas: 0
        Missing blocks: 0
        Missing blocks (with replication factor 1): 0
        Low redundancy blocks with highest priority to recover: 0
        Pending deletion blocks: 0
Erasure Coded Block Groups:
        Low redundancy block groups: 0
        Block groups with corrupt internal blocks: 0
        Missing block groups: 0
        Low redundancy blocks with highest priority to recover: 0
        Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (3):

Name: 192.168.0.102:9866 (bie2)
Hostname: bie2
Decommission Status : Normal
Configured Capacity: 105088192512 (97.87 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 45056 (44 KB)
DFS Remaining: 99702693888 (92.86 GB)
DFS Used%: 0.00%
DFS Remaining%: 94.88%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Sat Feb 17 20:22:26 UTC 2024
Last Block Report: Sat Feb 17 20:19:59 UTC 2024
Num of Blocks: 0


Name: 192.168.0.103:9866 (bie3)
Hostname: bie3
Decommission Status : Normal
Configured Capacity: 105088192512 (97.87 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 45056 (44 KB)
DFS Remaining: 99702693888 (92.86 GB)
DFS Used%: 0.00%
DFS Remaining%: 94.88%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Sat Feb 17 20:22:25 UTC 2024
Last Block Report: Sat Feb 17 20:19:34 UTC 2024
Num of Blocks: 0


Name: 192.168.0.104:9866 (bie4)
Hostname: bie4
Decommission Status : Normal
Configured Capacity: 105088192512 (97.87 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 45056 (44 KB)
DFS Remaining: 99702693888 (92.86 GB)
DFS Used%: 0.00%
DFS Remaining%: 94.88%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Sat Feb 17 20:22:25 UTC 2024
Last Block Report: Sat Feb 17 20:20:10 UTC 2024
Num of Blocks: 0
```

<!-- pagebreak -->

 env-start.sh

![env-start.sh](env-start.sh)

<!-- pagebreak -->

 env-stop.sh

![env-stop.sh](env-stop.sh)
